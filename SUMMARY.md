# Summary

## Introduction

* [Introduction](README.md)
* [Vision-based SLAM](vision-based-slam.md)
* [Paper\_2017\_Survey\_CV4Vehicle \(50%\)](paper2017-survey.md)
* [Paper\_2017\_Overview of Services \(50%\)](paper2017-overview-of-services.md)
* [Paper\_2013\_Survey\_Vehicle Detection ](paper2013-survey-vehicle-detection.md)

## 2D CNN

* [Paper\_2017\_SqueezeDet](paper2016-squeezedet.md)

## 2.3D 2D to 3D Back-Projection

* [Paper\_국내 논문\_단일카메라](paperdepth-from-single-image/paper2015-b2e8-c77c-ce74-ba54-b77c-2-c7a5-c758-c774-bbf8-c9c0.md)
  * [Lab\_Depth Map from Stereo Images](paperdepth-from-single-image/paper2015-b2e8-c77c-ce74-ba54-b77c-2-c7a5-c758-c774-bbf8-c9c0/labdepth-map-from-stereo-images.md)
* [Paper\_2014\_SPS-Stereo \(30%\)](paper2014-sps-stereo.md)
* [Paper\_2016\_Monocular Depth /w 2Camera](paper2016-monocular-depth.md)
* [Paper\_2016\_Monocular 3D \(70%\)](papermonocular-3d.md)
* [Paper\_2017\_3DOP\_X Chen \(70%\)](paper2017-3d-object-proposals.md)
* [Paper\_2017\_Deep3DBox](paper2017-3d-bbox.md)
* [Paper\_2017\_Depth dilated DNN \(50%\)](paperdepth-from-single-image.md)
* [Report\_2017\_Monocular 3 CNN Methods \(70%\)](report2017-monocular-3-cnnmethods.md)

## 3D PointCloud

* [Intro\_3D CloudPoint](intro3d-cloudpoint.md)
* [Paper\_2017\_Sruvey\_3D data\(100%\)](paper2017-sruvey-3d-data.md)
* [Paper\_2016\_Vote3Deep \(10%\)](papervote3deep.md)
* [Paper\_2016\_VeloFCN\_Bo Li \(30%\)](paper2016-velofcn4vd.md)
* [Paper\_2017\_FCN4VD\_Bo Li  \(30%\)](paper3d-cnn.md)
* [Paper\_2016\_3D GAN](paper2016-3d-gan.md)
* [Paper\_2017\_LoDNN\_Road Detection \(10%\)](paper2017-lodnnroad-detection.md)
* [Paper\_2017\_Instant Object Detection \(30%\)](paper2017-instant-object-detection.md)
* [Paper\_2017\_OctNet \(30%\)](paper2017-octnet.md)
* [Paper\_2017\_SqueezeSeg](paper2017-squeezeseg.md)
* [Intro\_3D\_RGB-D](intro3d-rgb-d.md)
* [Paper\_2015\_VoxNet \(10%\)](papervoxnet.md)
* [Paper\_2016\_Volumetric\_Multiview CNNs \(50%\)](paper2016-volumetric-multiview-cnns.md)
* [Paper\_2016\_DeepSlidingShape \(30%\)](paper2016-deepslidingshape.md)
* [Paper\_2017\_PointNet \(10%\)](paper2016-pointnet.md)
* [Point\_Cloud\_Data](pointcloud-data.md)
  * [ROS bags-TO-Image.ipynb](https://gist.github.com/anonymous/4857f8920c9fc901121a429ead32a7db)
  * [ROS bags-TO-Point Clods.ipynb](https://gist.github.com/anonymous/e675ea14113252be321320be62248034)
  * [ROS bags-TO-Avi.ipynb](https://gist.github.com/anonymous/fb1e98efe187b2a35b6d91fb5df9e83b)
  * [KITTI Dataset Exploration.ipynb](https://github.com/hunjung-lim/awesome-vehicle-datasets/blob/master/vehicle/kitti/KITTI%2BDataset%2BExploration.ipynb)
  * [KITTI Dataset Visualizing.ipynb](https://github.com/hunjung-lim/awesome-vehicle-datasets/blob/master/vehicle/kitti/KITTI%2BDataset%2BVisualizing.ipynb)
  * [Read\_RGBD](pointcloud-data/readrgbd.md)
  * [Velodyne\_LiDAR](pointcloud-data/velodynelidar.md)

## Fusion

* [Intro\_Fusion](introfusion.md)
* [Paper\_2017\_MV3D\(70%\)](papermultiview-3d-cnn.md)
  * [Code\_MV3D](papermultiview-3d-cnn/codemv3d.md)
  * [code\_MV3D\_TF](papermultiview-3d-cnn/codemv3d-tf.md)
* [Paper\_2016\_FusionNet \(50%\)](paper2016-fusionnet.md)
* [Paper\_2016\_Fusing\_LIDAR\_IMAGE\_Pedestrian \(30%\)](paper2016-fusing-lidar-image-pedestrian.md)

## 4D \(Time, RNN\)

* [Paper\_2016\_3D-R2N2](paper2016-3d-r2n2.md)

## 참고

* [ref01\_Hardware](ref01hardware.md)
* [ref02\_Metrics](ref02metrics.md)
* [ref03\_Tracklet](ref03tracklet.md)
* ref04\_non-maxima suppression \(NMS\)

