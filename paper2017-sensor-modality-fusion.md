| 논문명 | Sensor Modality Fusion with CNNs for UGV Autonomous Driving in Indoor Environments |
| --- | --- |
| 저자\(소속\) | Naman Patel\(NYU\) |
| 학회/년도 | 2017, [논문](http://cims.nyu.edu/~achoroma/NonFlash/Papers/SMF_CNN.pdf) |
| 키워드 |   |
| 데이터셋(센서)/모델 | own dataset(LiDAR + Camera) |
| 참고 |  |
| 코드 |  |

# SMF_CNN

- We present a novel end-to-end learning framework by fusing **raw pixels** from cameras and depth measurements from a **LiDAR**. 

- **A deep neural network** architecture is introduced to effectively perform **modality fusion** and reliably **predict steering commands** even in the presence of sensor failures. 

## I. INTRODUCTION

Our work focuses on the problem of navigating of an
autonomous unmanned ground vehicle (UGV) using vision
and depth measurements in an indoor environment. We
propose a deep learning architecture for the sensor fusion
problem that consists of two convolutional neural networks
(CNNs), each consisting of a different input modality, which
are fused with a gating mechanism. 


<!--stackedit_data:
eyJoaXN0b3J5IjpbLTU0Mjg3MTc4XX0=
-->