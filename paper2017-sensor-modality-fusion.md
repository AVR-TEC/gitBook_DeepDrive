| 논문명 | Sensor Modality Fusion with CNNs for UGV Autonomous Driving in Indoor Environments |
| --- | --- |
| 저자\(소속\) | Naman Patel\(NYU\) |
| 학회/년도 | 2017, [논문](http://cims.nyu.edu/~achoroma/NonFlash/Papers/SMF_CNN.pdf) |
| 키워드 |   |
| 데이터셋(센서)/모델 | |
| 참고 |  |
| 코드 |  |

# SMF_CNN

- We present a novel end-to-end learning framework by fusing **raw pixels** from cameras and depth measurements from a **LiDAR**. 

- A deep neural network architecture is introduced to effectively perform modality fusion and reliably predict steering commands even in the presence of sensor failures. 

The proposed network is trained on our owndataset, from LiDAR and a camera mounted on a UGV taken inan indoor corridor environment. 

Comprehensive experimentalevaluation to demonstrate the robustness of our network architectureis performed to show that the proposed deep learningneural network is able to autonomously navigate in the corridorenvironment. 

Furthermore, we demonstrate that the fusion ofthe camera and LiDAR modalities provides further benefitsbeyond robustness to sensor failures. 

Specifically, the multimodalfused system shows a potential to navigate around staticand dynamic obstacles and to handle changes in environmentgeometry without being trained for these tasks.
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTcxNjI1Mzc3NF19
-->